{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model Sampling - FlowBending Framework\n",
    "\n",
    "This notebook implements hybrid video generation using both Wan 14B and 1.3B models with configurable sampling schedules.\n",
    "\n",
    "## Approach:\n",
    "- Load both 14B and 1.3B models with shared VAE/T5\n",
    "- Implement flexible sampling schedule (e.g., LSSSL pattern)\n",
    "- Work in latent space to maintain consistency during model switching\n",
    "- Profile memory usage, latency, and total time per segment\n",
    "- Compare with 14B-only baseline\n",
    "\n",
    "## Resolution: 480p (832×480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "CUDA available: True\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '/workspace/wan2.1/Wan2.1')\n",
    "\n",
    "from wan.text2video import WanT2V\n",
    "from wan.configs.wan_t2v_14B import t2v_14B\n",
    "from wan.configs.wan_t2v_1_3B import t2v_1_3B\n",
    "from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler\n",
    "from wan.utils.fm_solvers import (\n",
    "    FlowDPMSolverMultistepScheduler,\n",
    "    get_sampling_sigmas,\n",
    "    retrieve_timesteps,\n",
    ")\n",
    "from wan.utils.utils import cache_video\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  checkpoint_dir_14B: /workspace/wan2.1/Wan2.1/Wan2.1-T2V-14B\n",
      "  checkpoint_dir_1_3B: /workspace/wan2.1/Wan2.1/Wan2.1-T2V-1.3B\n",
      "  output_dir: /workspace/wan2.1/Wan2.1/outputs\n",
      "  width: 832\n",
      "  height: 480\n",
      "  frame_num: 81\n",
      "  total_sampling_steps: 50\n",
      "  sample_solver: unipc\n",
      "  shift: 5.0\n",
      "  guide_scale: 5.0\n",
      "  prompt: Two anthropomorphic cats in comfy boxing gear sparring playfully in a cozy living room\n",
      "  negative_prompt: \n",
      "  seed: 42\n",
      "  device_id: 0\n",
      "  offload_models: False\n",
      "  keep_both_loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Basic Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'checkpoint_dir_14B': '/workspace/wan2.1/Wan2.1/Wan2.1-T2V-14B',\n",
    "    'checkpoint_dir_1_3B': '/workspace/wan2.1/Wan2.1/Wan2.1-T2V-1.3B',\n",
    "    'output_dir': '/workspace/wan2.1/Wan2.1/outputs',\n",
    "    \n",
    "    # Resolution (480p)\n",
    "    'width': 832,\n",
    "    'height': 480,\n",
    "    'frame_num': 81,  # Must be 4n+1\n",
    "    \n",
    "    # Sampling parameters\n",
    "    'total_sampling_steps': 50,\n",
    "    'sample_solver': 'unipc',  # 'unipc' or 'dpm++'\n",
    "    'shift': 5.0,\n",
    "    'guide_scale': 5.0,\n",
    "    \n",
    "    # Prompt\n",
    "    'prompt': 'Two anthropomorphic cats in comfy boxing gear sparring playfully in a cozy living room',\n",
    "    'negative_prompt': '',  # Will use default if empty\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Device\n",
    "    'device_id': 0,\n",
    "    \n",
    "    # Model management\n",
    "    'offload_models': False,  # Set to True to offload models between segments (saves memory)\n",
    "    'keep_both_loaded': True,  # Set to False if running into memory issues\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Schedule Configuration\n",
    "\n",
    "Configure the hybrid sampling schedule here. Define which steps use which model.\n",
    "\n",
    "**Patterns:**\n",
    "- **LSL**: Large → Small → Large (e.g., 3-44-3 for 50 steps)\n",
    "- **LSSSL**: Large → Small → Small → Small → Large (e.g., 10-10-10-10-10)\n",
    "- **Custom**: Define your own segment boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Schedule:\n",
      "--------------------------------------------------\n",
      "Segment 1: Steps  0- 3 ( 3 steps) → 14B\n",
      "Segment 2: Steps  3-47 (44 steps) → 1.3B\n",
      "Segment 3: Steps 47-50 ( 3 steps) → 14B\n",
      "--------------------------------------------------\n",
      "Total: 50 steps\n"
     ]
    }
   ],
   "source": [
    "# Sampling Schedule Configuration\n",
    "# Define segments: list of (model_name, num_steps)\n",
    "# model_name: '14B' or '1.3B'\n",
    "\n",
    "# Example patterns (uncomment one or create your own):\n",
    "\n",
    "# Pattern 1: LSL (3-44-3) - Original request\n",
    "SAMPLING_SCHEDULE = [\n",
    "    ('14B', 5),    # First 3 steps with 14B\n",
    "    ('1.3B', 20), \n",
    "    '14B', 5),\n",
    "    ('1.3B', 20), # Next 44 steps with 1.3B\n",
    "    ('14B', 5),    # Last 3 steps with 14B\n",
    "]\n",
    "\n",
    "# Pattern 2: LSSSL (10-10-10-10-10)\n",
    "# SAMPLING_SCHEDULE = [\n",
    "#     ('14B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('14B', 10),\n",
    "# ]\n",
    "\n",
    "# Pattern 3: Heavy start and end (5-40-5)\n",
    "# SAMPLING_SCHEDULE = [\n",
    "#     ('14B', 5),\n",
    "#     ('1.3B', 40),\n",
    "#     ('14B', 5),\n",
    "# ]\n",
    "\n",
    "# Pattern 4: Multiple switches (5-10-10-10-10-5)\n",
    "# SAMPLING_SCHEDULE = [\n",
    "#     ('14B', 5),\n",
    "#     ('1.3B', 10),\n",
    "#     ('14B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('14B', 10),\n",
    "#     ('1.3B', 5),\n",
    "# ]\n",
    "\n",
    "# Validate schedule\n",
    "total_steps_scheduled = sum(steps for _, steps in SAMPLING_SCHEDULE)\n",
    "assert total_steps_scheduled == CONFIG['total_sampling_steps'], \\\n",
    "    f\"Schedule steps ({total_steps_scheduled}) must match total_sampling_steps ({CONFIG['total_sampling_steps']})\"\n",
    "\n",
    "print(\"Sampling Schedule:\")\n",
    "print(\"-\" * 50)\n",
    "cumulative = 0\n",
    "for i, (model, steps) in enumerate(SAMPLING_SCHEDULE):\n",
    "    start_step = cumulative\n",
    "    end_step = cumulative + steps\n",
    "    cumulative = end_step\n",
    "    print(f\"Segment {i+1}: Steps {start_step:2d}-{end_step:2d} ({steps:2d} steps) → {model}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total: {total_steps_scheduled} steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Utility Functions for Profiling\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in GB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / 1e9,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1e9,\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / 1e9,\n",
    "        }\n",
    "    return {'allocated': 0, 'reserved': 0, 'max_allocated': 0}\n",
    "\n",
    "def reset_peak_memory():\n",
    "    \"\"\"Reset peak memory stats\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "class SegmentProfiler:\n",
    "    \"\"\"Profile a sampling segment\"\"\"\n",
    "    def __init__(self, segment_name: str):\n",
    "        self.segment_name = segment_name\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.start_memory = None\n",
    "        self.peak_memory = None\n",
    "        self.step_times = []\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start profiling\"\"\"\n",
    "        reset_peak_memory()\n",
    "        self.start_time = time.time()\n",
    "        self.start_memory = get_gpu_memory()\n",
    "        \n",
    "    def record_step(self, step_time: float):\n",
    "        \"\"\"Record time for a single step\"\"\"\n",
    "        self.step_times.append(step_time)\n",
    "        \n",
    "    def end(self):\n",
    "        \"\"\"End profiling\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        self.peak_memory = get_gpu_memory()\n",
    "        \n",
    "    def get_report(self) -> Dict:\n",
    "        \"\"\"Get profiling report\"\"\"\n",
    "        total_time = self.end_time - self.start_time if self.end_time else 0\n",
    "        avg_step_time = np.mean(self.step_times) if self.step_times else 0\n",
    "        \n",
    "        return {\n",
    "            'segment_name': self.segment_name,\n",
    "            'num_steps': len(self.step_times),\n",
    "            'total_time': total_time,\n",
    "            'avg_step_time': avg_step_time,\n",
    "            'min_step_time': np.min(self.step_times) if self.step_times else 0,\n",
    "            'max_step_time': np.max(self.step_times) if self.step_times else 0,\n",
    "            'memory_start_allocated_gb': self.start_memory['allocated'],\n",
    "            'memory_peak_allocated_gb': self.peak_memory['max_allocated'],\n",
    "            'memory_peak_reserved_gb': self.peak_memory['reserved'],\n",
    "        }\n",
    "        \n",
    "    def print_report(self):\n",
    "        \"\"\"Print formatted report\"\"\"\n",
    "        report = self.get_report()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Segment: {report['segment_name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Steps: {report['num_steps']}\")\n",
    "        print(f\"Total time: {report['total_time']:.2f}s\")\n",
    "        print(f\"Avg step time: {report['avg_step_time']:.3f}s\")\n",
    "        print(f\"Min step time: {report['min_step_time']:.3f}s\")\n",
    "        print(f\"Max step time: {report['max_step_time']:.3f}s\")\n",
    "        print(f\"Memory (start): {report['memory_start_allocated_gb']:.2f} GB\")\n",
    "        print(f\"Memory (peak): {report['memory_peak_allocated_gb']:.2f} GB\")\n",
    "        print(f\"Memory (reserved): {report['memory_peak_reserved_gb']:.2f} GB\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"✓ Utility functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "Load both 14B and 1.3B models. They share the same VAE and T5 encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 14B model...\n",
      "✓ 14B model loaded\n",
      "Memory after 14B: 57.68 GB\n",
      "\n",
      "Loading 1.3B model...\n",
      "✓ 1.3B model loaded\n",
      "Memory after 1.3B: 63.89 GB\n",
      "\n",
      "✓ Both models loaded. Total memory: 63.89 GB\n"
     ]
    }
   ],
   "source": [
    "# Load 14B Model\n",
    "print(\"Loading 14B model...\")\n",
    "model_14B = WanT2V(\n",
    "    config=t2v_14B,\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir_14B'],\n",
    "    device_id=CONFIG['device_id'],\n",
    "    rank=0,\n",
    "    t5_fsdp=False,\n",
    "    dit_fsdp=False,\n",
    "    use_usp=False,\n",
    "    t5_cpu=False,\n",
    ")\n",
    "print(\"✓ 14B model loaded\")\n",
    "print(f\"Memory after 14B: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "\n",
    "# Load 1.3B Model\n",
    "print(\"\\nLoading 1.3B model...\")\n",
    "model_1_3B = WanT2V(\n",
    "    config=t2v_1_3B,\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir_1_3B'],\n",
    "    device_id=CONFIG['device_id'],\n",
    "    rank=0,\n",
    "    t5_fsdp=False,\n",
    "    dit_fsdp=False,\n",
    "    use_usp=False,\n",
    "    t5_cpu=False,\n",
    ")\n",
    "print(\"✓ 1.3B model loaded\")\n",
    "print(f\"Memory after 1.3B: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "\n",
    "# Store models in a dictionary for easy access\n",
    "models = {\n",
    "    '14B': model_14B,\n",
    "    '1.3B': model_1_3B,\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Both models loaded. Total memory: {get_gpu_memory()['allocated']:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Sampling\n",
    "\n",
    "FlowBending-inspired hybrid sampling with model switching in latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hybrid sampling function defined\n"
     ]
    }
   ],
   "source": [
    "def hybrid_generate(\n",
    "    models: Dict,\n",
    "    sampling_schedule: List[Tuple[str, int]],\n",
    "    config: Dict,\n",
    "    profile: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate video using hybrid model sampling.\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of models {'14B': model_14B, '1.3B': model_1_3B}\n",
    "        sampling_schedule: List of (model_name, num_steps) tuples\n",
    "        config: Configuration dictionary\n",
    "        profile: Whether to profile each segment\n",
    "        \n",
    "    Returns:\n",
    "        video: Generated video tensor\n",
    "        profiling_reports: List of profiling reports for each segment\n",
    "    \"\"\"\n",
    "    device = torch.device(f\"cuda:{config['device_id']}\")\n",
    "    \n",
    "    # Use first model to get shared components\n",
    "    first_model = models[sampling_schedule[0][0]]\n",
    "    \n",
    "    # Prepare latent shape\n",
    "    F = config['frame_num']\n",
    "    size = (config['width'], config['height'])\n",
    "    vae_stride = first_model.vae_stride\n",
    "    patch_size = first_model.patch_size\n",
    "    \n",
    "    target_shape = (\n",
    "        first_model.vae.model.z_dim,\n",
    "        (F - 1) // vae_stride[0] + 1,\n",
    "        size[1] // vae_stride[1],\n",
    "        size[0] // vae_stride[2]\n",
    "    )\n",
    "    \n",
    "    seq_len = math.ceil(\n",
    "        (target_shape[2] * target_shape[3]) / (patch_size[1] * patch_size[2]) * target_shape[1]\n",
    "    )\n",
    "    \n",
    "    # Setup text encoding (shared across all models)\n",
    "    n_prompt = config['negative_prompt'] if config['negative_prompt'] else first_model.sample_neg_prompt\n",
    "    seed = config['seed'] if config['seed'] >= 0 else random.randint(0, sys.maxsize)\n",
    "    \n",
    "    print(f\"Using seed: {seed}\")\n",
    "    print(f\"Target latent shape: {target_shape}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    \n",
    "    # Encode text prompt (use T5 from first model)\n",
    "    first_model.text_encoder.model.to(device)\n",
    "    context = first_model.text_encoder([config['prompt']], device)\n",
    "    context_null = first_model.text_encoder([n_prompt], device)\n",
    "    first_model.text_encoder.model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize noise with seed\n",
    "    seed_g = torch.Generator(device=device)\n",
    "    seed_g.manual_seed(seed)\n",
    "    \n",
    "    noise = torch.randn(\n",
    "        target_shape[0], target_shape[1], target_shape[2], target_shape[3],\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "        generator=seed_g\n",
    "    )\n",
    "    \n",
    "    # Setup scheduler\n",
    "    num_train_timesteps = first_model.num_train_timesteps\n",
    "    if config['sample_solver'] == 'unipc':\n",
    "        sample_scheduler = FlowUniPCMultistepScheduler(\n",
    "            num_train_timesteps=num_train_timesteps,\n",
    "            shift=1,\n",
    "            use_dynamic_shifting=False\n",
    "        )\n",
    "        sample_scheduler.set_timesteps(config['total_sampling_steps'], device=device, shift=config['shift'])\n",
    "        timesteps = sample_scheduler.timesteps\n",
    "    elif config['sample_solver'] == 'dpm++':\n",
    "        sample_scheduler = FlowDPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=num_train_timesteps,\n",
    "            shift=1,\n",
    "            use_dynamic_shifting=False\n",
    "        )\n",
    "        sampling_sigmas = get_sampling_sigmas(config['total_sampling_steps'], config['shift'])\n",
    "        timesteps, _ = retrieve_timesteps(sample_scheduler, device=device, sigmas=sampling_sigmas)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported solver: {config['sample_solver']}\")\n",
    "    \n",
    "    print(f\"Total timesteps: {len(timesteps)}\")\n",
    "    \n",
    "    # Initialize latents\n",
    "    latents = noise\n",
    "    \n",
    "    # Prepare model arguments\n",
    "    arg_c = {'context': context, 'seq_len': seq_len}\n",
    "    arg_null = {'context': context_null, 'seq_len': seq_len}\n",
    "    \n",
    "    # Run hybrid sampling\n",
    "    profiling_reports = []\n",
    "    step_idx = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYBRID SAMPLING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for segment_idx, (model_name, num_steps) in enumerate(sampling_schedule):\n",
    "        model = models[model_name]\n",
    "        segment_name = f\"Segment {segment_idx+1}: {model_name} ({num_steps} steps)\"\n",
    "        \n",
    "        print(f\"\\n{segment_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Setup profiler\n",
    "        profiler = SegmentProfiler(segment_name) if profile else None\n",
    "        if profiler:\n",
    "            profiler.start()\n",
    "        \n",
    "        # Move model to device if needed\n",
    "        model.model.to(device)\n",
    "        \n",
    "        # Sample for this segment\n",
    "        segment_timesteps = timesteps[step_idx:step_idx + num_steps]\n",
    "        \n",
    "        with amp.autocast(dtype=model.param_dtype), torch.no_grad():\n",
    "            for i, t in enumerate(tqdm(segment_timesteps, desc=model_name)):\n",
    "                step_start = time.time()\n",
    "                \n",
    "                latent_model_input = [latents]\n",
    "                timestep = torch.stack([t])\n",
    "                \n",
    "                # Conditional prediction\n",
    "                noise_pred_cond = model.model(latent_model_input, t=timestep, **arg_c)[0]\n",
    "                # Unconditional prediction\n",
    "                noise_pred_uncond = model.model(latent_model_input, t=timestep, **arg_null)[0]\n",
    "                \n",
    "                # Classifier-free guidance\n",
    "                noise_pred = noise_pred_uncond + config['guide_scale'] * (noise_pred_cond - noise_pred_uncond)\n",
    "                \n",
    "                # Scheduler step\n",
    "                temp_x0 = sample_scheduler.step(\n",
    "                    noise_pred.unsqueeze(0),\n",
    "                    t,\n",
    "                    latents.unsqueeze(0),\n",
    "                    return_dict=False,\n",
    "                    generator=seed_g\n",
    "                )[0]\n",
    "                latents = temp_x0.squeeze(0)\n",
    "                \n",
    "                step_time = time.time() - step_start\n",
    "                if profiler:\n",
    "                    profiler.record_step(step_time)\n",
    "        \n",
    "        # End profiling\n",
    "        if profiler:\n",
    "            profiler.end()\n",
    "            profiler.print_report()\n",
    "            profiling_reports.append(profiler.get_report())\n",
    "        \n",
    "        # Offload model if requested\n",
    "        if config['offload_models']:\n",
    "            model.model.cpu()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Offloaded {model_name} model\")\n",
    "        \n",
    "        step_idx += num_steps\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DECODING LATENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Decode latents using VAE (from any model, they're the same)\n",
    "    decode_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        videos = first_model.vae.decode([latents])\n",
    "    decode_time = time.time() - decode_start\n",
    "    print(f\"Decode time: {decode_time:.2f}s\")\n",
    "    \n",
    "    return videos[0], profiling_reports\n",
    "\n",
    "print(\"✓ Hybrid sampling function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hybrid Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hybrid generation...\n",
      "Prompt: Two anthropomorphic cats in comfy boxing gear sparring playfully in a cozy living room\n",
      "Resolution: 832x480\n",
      "Frames: 81\n",
      "Using seed: 42\n",
      "Target latent shape: (16, 21, 60, 104)\n",
      "Sequence length: 32760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14007/529248829.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(dtype=model.param_dtype), torch.no_grad():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 50\n",
      "\n",
      "================================================================================\n",
      "HYBRID SAMPLING\n",
      "================================================================================\n",
      "\n",
      "Segment 1: 14B (3 steps)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14B: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Segment: Segment 1: 14B (3 steps)\n",
      "============================================================\n",
      "Steps: 3\n",
      "Total time: 20.08s\n",
      "Avg step time: 6.690s\n",
      "Min step time: 6.682s\n",
      "Max step time: 6.698s\n",
      "Memory (start): 64.70 GB\n",
      "Memory (peak): 71.84 GB\n",
      "Memory (reserved): 73.89 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "Segment 2: 1.3B (44 steps)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.3B: 100%|██████████| 44/44 [01:03<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Segment: Segment 2: 1.3B (44 steps)\n",
      "============================================================\n",
      "Steps: 44\n",
      "Total time: 63.30s\n",
      "Avg step time: 1.438s\n",
      "Min step time: 1.437s\n",
      "Max step time: 1.439s\n",
      "Memory (start): 64.76 GB\n",
      "Memory (peak): 66.90 GB\n",
      "Memory (reserved): 73.89 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "Segment 3: 14B (3 steps)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14B: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Segment: Segment 3: 14B (3 steps)\n",
      "============================================================\n",
      "Steps: 3\n",
      "Total time: 20.07s\n",
      "Avg step time: 6.687s\n",
      "Min step time: 6.686s\n",
      "Max step time: 6.688s\n",
      "Memory (start): 64.76 GB\n",
      "Memory (peak): 71.84 GB\n",
      "Memory (reserved): 73.89 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DECODING LATENTS\n",
      "================================================================================\n",
      "Decode time: 1.43s\n",
      "\n",
      "✓ Hybrid video saved to: /workspace/wan2.1/Wan2.1/outputs/hybrid_832x480_20260106_080506.mp4\n"
     ]
    }
   ],
   "source": [
    "# Run Hybrid Generation\n",
    "print(\"Starting hybrid generation...\")\n",
    "print(f\"Prompt: {CONFIG['prompt']}\")\n",
    "print(f\"Resolution: {CONFIG['width']}x{CONFIG['height']}\")\n",
    "print(f\"Frames: {CONFIG['frame_num']}\")\n",
    "\n",
    "hybrid_video, hybrid_reports = hybrid_generate(\n",
    "    models=models,\n",
    "    sampling_schedule=SAMPLING_SCHEDULE,\n",
    "    config=CONFIG,\n",
    "    profile=True\n",
    ")\n",
    "\n",
    "# Save hybrid video\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hybrid_output_path = os.path.join(\n",
    "    CONFIG['output_dir'],\n",
    "    f\"hybrid_{CONFIG['width']}x{CONFIG['height']}_{timestamp}.mp4\"\n",
    ")\n",
    "cache_video(hybrid_video[None], save_file=hybrid_output_path, fps=16, nrow=1, normalize=True, value_range=(-1, 1))\n",
    "print(f\"\\n✓ Hybrid video saved to: {hybrid_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: 14B-Only Generation\n",
    "\n",
    "Run full 50-step generation with 14B model only for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 14B baseline generation...\n",
      "Prompt: Two anthropomorphic cats in comfy boxing gear sparring playfully in a cozy living room\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:35<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 14B baseline completed in 339.00s\n",
      "✓ Baseline video saved to: /workspace/wan2.1/Wan2.1/outputs/baseline_14B_832x480_20260106_080506.mp4\n"
     ]
    }
   ],
   "source": [
    "# Generate baseline with 14B only\n",
    "print(\"Starting 14B baseline generation...\")\n",
    "print(f\"Prompt: {CONFIG['prompt']}\")\n",
    "\n",
    "baseline_start = time.time()\n",
    "\n",
    "baseline_video = model_14B.generate(\n",
    "    input_prompt=CONFIG['prompt'],\n",
    "    size=(CONFIG['width'], CONFIG['height']),\n",
    "    frame_num=CONFIG['frame_num'],\n",
    "    shift=CONFIG['shift'],\n",
    "    sample_solver=CONFIG['sample_solver'],\n",
    "    sampling_steps=CONFIG['total_sampling_steps'],\n",
    "    guide_scale=CONFIG['guide_scale'],\n",
    "    n_prompt=CONFIG['negative_prompt'],\n",
    "    seed=CONFIG['seed'],\n",
    "    offload_model=CONFIG['offload_models']\n",
    ")\n",
    "\n",
    "baseline_time = time.time() - baseline_start\n",
    "print(f\"\\n✓ 14B baseline completed in {baseline_time:.2f}s\")\n",
    "\n",
    "# Save baseline video\n",
    "baseline_output_path = os.path.join(\n",
    "    CONFIG['output_dir'],\n",
    "    f\"baseline_14B_{CONFIG['width']}x{CONFIG['height']}_{timestamp}.mp4\"\n",
    ")\n",
    "cache_video(baseline_video[None], save_file=baseline_output_path, fps=16, nrow=1, normalize=True, value_range=(-1, 1))\n",
    "print(f\"✓ Baseline video saved to: {baseline_output_path}\")\n",
    "\n",
    "# Create baseline report\n",
    "baseline_report = {\n",
    "    'model': '14B only',\n",
    "    'total_steps': CONFIG['total_sampling_steps'],\n",
    "    'total_time': baseline_time,\n",
    "    'avg_step_time': baseline_time / CONFIG['total_sampling_steps'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Report\n",
    "\n",
    "Generate comprehensive profiling report comparing hybrid vs baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROFILING REPORT - HYBRID VS BASELINE\n",
      "================================================================================\n",
      "\n",
      "### HYBRID MODEL SAMPLING ###\n",
      "Schedule: [('14B', 3), ('1.3B', 44), ('14B', 3)]\n",
      "\n",
      "Segment Details:\n",
      "\n",
      "  Segment 1: 14B (3 steps)\n",
      "    Steps: 3\n",
      "    Total time: 20.08s\n",
      "    Avg step time: 6.690s\n",
      "    Peak memory: 71.84 GB\n",
      "\n",
      "  Segment 2: 1.3B (44 steps)\n",
      "    Steps: 44\n",
      "    Total time: 63.30s\n",
      "    Avg step time: 1.438s\n",
      "    Peak memory: 66.90 GB\n",
      "\n",
      "  Segment 3: 14B (3 steps)\n",
      "    Steps: 3\n",
      "    Total time: 20.07s\n",
      "    Avg step time: 6.687s\n",
      "    Peak memory: 71.84 GB\n",
      "\n",
      "  Hybrid Total Time: 103.46s\n",
      "  Hybrid Avg Step Time: 2.069s\n",
      "\n",
      "### BASELINE (14B ONLY) ###\n",
      "Total time: 339.00s\n",
      "Avg step time: 6.780s\n",
      "\n",
      "### COMPARISON ###\n",
      "Speedup: 3.28x\n",
      "Time saved: 235.55s\n",
      "Percentage faster: 69.5%\n",
      "\n",
      "✓ Report saved to: /workspace/wan2.1/Wan2.1/outputs/profiling_report_20260106_080506.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROFILING REPORT - HYBRID VS BASELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hybrid summary\n",
    "print(\"\\n### HYBRID MODEL SAMPLING ###\")\n",
    "print(f\"Schedule: {SAMPLING_SCHEDULE}\")\n",
    "print(f\"\\nSegment Details:\")\n",
    "hybrid_total_time = 0\n",
    "for report in hybrid_reports:\n",
    "    print(f\"\\n  {report['segment_name']}\")\n",
    "    print(f\"    Steps: {report['num_steps']}\")\n",
    "    print(f\"    Total time: {report['total_time']:.2f}s\")\n",
    "    print(f\"    Avg step time: {report['avg_step_time']:.3f}s\")\n",
    "    print(f\"    Peak memory: {report['memory_peak_allocated_gb']:.2f} GB\")\n",
    "    hybrid_total_time += report['total_time']\n",
    "\n",
    "print(f\"\\n  Hybrid Total Time: {hybrid_total_time:.2f}s\")\n",
    "print(f\"  Hybrid Avg Step Time: {hybrid_total_time / CONFIG['total_sampling_steps']:.3f}s\")\n",
    "\n",
    "# Baseline summary\n",
    "print(\"\\n### BASELINE (14B ONLY) ###\")\n",
    "print(f\"Total time: {baseline_report['total_time']:.2f}s\")\n",
    "print(f\"Avg step time: {baseline_report['avg_step_time']:.3f}s\")\n",
    "\n",
    "# Comparison\n",
    "speedup = baseline_report['total_time'] / hybrid_total_time\n",
    "print(\"\\n### COMPARISON ###\")\n",
    "print(f\"Speedup: {speedup:.2f}x\")\n",
    "print(f\"Time saved: {baseline_report['total_time'] - hybrid_total_time:.2f}s\")\n",
    "print(f\"Percentage faster: {(1 - 1/speedup) * 100:.1f}%\")\n",
    "\n",
    "# Save report to JSON\n",
    "report_data = {\n",
    "    'timestamp': timestamp,\n",
    "    'config': CONFIG,\n",
    "    'sampling_schedule': SAMPLING_SCHEDULE,\n",
    "    'hybrid': {\n",
    "        'segments': hybrid_reports,\n",
    "        'total_time': hybrid_total_time,\n",
    "        'avg_step_time': hybrid_total_time / CONFIG['total_sampling_steps'],\n",
    "    },\n",
    "    'baseline': baseline_report,\n",
    "    'comparison': {\n",
    "        'speedup': speedup,\n",
    "        'time_saved': baseline_report['total_time'] - hybrid_total_time,\n",
    "        'percentage_faster': (1 - 1/speedup) * 100,\n",
    "    },\n",
    "    'outputs': {\n",
    "        'hybrid_video': hybrid_output_path,\n",
    "        'baseline_video': baseline_output_path,\n",
    "    }\n",
    "}\n",
    "\n",
    "report_path = os.path.join(CONFIG['output_dir'], f'profiling_report_{timestamp}.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Report saved to: {report_path}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Cleanup\n",
    "\n",
    "Note: This cell is NOT run by default. Execute manually to clean up memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Cleanup Cell\n",
    "# Set this to True and run manually when you want to clean up\n",
    "RUN_CLEANUP = False\n",
    "\n",
    "if RUN_CLEANUP:\n",
    "    import gc\n",
    "    \n",
    "    print(\"Cleaning up memory...\")\n",
    "    print(f\"Memory before cleanup: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "    \n",
    "    # Delete models\n",
    "    if 'model_14B' in locals():\n",
    "        del model_14B\n",
    "    if 'model_1_3B' in locals():\n",
    "        del model_1_3B\n",
    "    if 'models' in locals():\n",
    "        del models\n",
    "    \n",
    "    # Delete videos\n",
    "    if 'hybrid_video' in locals():\n",
    "        del hybrid_video\n",
    "    if 'baseline_video' in locals():\n",
    "        del baseline_video\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Memory after cleanup: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "    print(\"✓ Cleanup complete\")\n",
    "else:\n",
    "    print(\"Cleanup skipped. Set RUN_CLEANUP = True and run this cell manually to clean up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements hybrid video generation using FlowBending framework:\n",
    "\n",
    "### Key Features:\n",
    "1. **Flexible Sampling Schedule**: Configure any pattern (LSL, LSSSL, custom)\n",
    "2. **Latent Space Consistency**: Models switch seamlessly in latent space\n",
    "3. **Comprehensive Profiling**: Memory usage, latency, and total time per segment\n",
    "4. **Baseline Comparison**: Full 14B generation for quality comparison\n",
    "\n",
    "### Outputs:\n",
    "- Hybrid video (saved to outputs directory)\n",
    "- Baseline 14B video (saved to outputs directory)\n",
    "- JSON profiling report with detailed metrics\n",
    "\n",
    "### Usage:\n",
    "1. Adjust `CONFIG` settings (resolution, prompt, seed, etc.)\n",
    "2. Modify `SAMPLING_SCHEDULE` to experiment with different patterns\n",
    "3. Run all cells sequentially\n",
    "4. Review profiling reports and compare videos\n",
    "5. (Optional) Run cleanup cell manually when done\n",
    "\n",
    "### Next Steps:\n",
    "- Compare video quality between hybrid and baseline\n",
    "- Experiment with different sampling schedules\n",
    "- Try different resolutions\n",
    "- Test with various prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wan2.1 .venv)",
   "language": "python",
   "name": "wan2.1-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
