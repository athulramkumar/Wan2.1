{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model Sampling - FlowBending Framework\n",
    "\n",
    "This notebook implements hybrid video generation using both Wan 14B and 1.3B models with configurable sampling schedules.\n",
    "\n",
    "## Approach:\n",
    "- Load both 14B and 1.3B models with shared VAE/T5\n",
    "- Implement flexible sampling schedule (e.g., LSSSL pattern)\n",
    "- Work in latent space to maintain consistency during model switching\n",
    "- Profile memory usage, latency, and total time per segment\n",
    "- Compare with 14B-only baseline\n",
    "\n",
    "## Resolution: 480p (832×480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "CUDA available: True\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '/workspace/wan2.1/Wan2.1')\n",
    "\n",
    "from wan.text2video import WanT2V\n",
    "from wan.configs.wan_t2v_14B import t2v_14B\n",
    "from wan.configs.wan_t2v_1_3B import t2v_1_3B\n",
    "from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler\n",
    "from wan.utils.fm_solvers import (\n",
    "    FlowDPMSolverMultistepScheduler,\n",
    "    get_sampling_sigmas,\n",
    "    retrieve_timesteps,\n",
    ")\n",
    "from wan.utils.utils import cache_video\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  checkpoint_dir_14B: /workspace/wan2.1/Wan2.1/Wan2.1-T2V-14B\n",
      "  checkpoint_dir_1_3B: /workspace/wan2.1/Wan2.1/Wan2.1-T2V-1.3B\n",
      "  output_dir: /workspace/wan2.1/Wan2.1/outputs\n",
      "  width: 832\n",
      "  height: 480\n",
      "  frame_num: 81\n",
      "  total_sampling_steps: 50\n",
      "  sample_solver: unipc\n",
      "  shift: 5.0\n",
      "  guide_scale: 5.0\n",
      "  prompt: A serene Japanese ukiyo-e woodblock print style animation. An elderly Zen monk in simple patched robes stands at the edge of frame, his back slightly turned, gazing upward at a luminous full moon in a beautiful night sky that dominates the upper portion of the composition. Behind him, the faint orange glow of dying embers marks where a small hut once stood—wisps of smoke curl lazily upward(smoke should be moving lightly), dissolving into the deep indigo night sky. The monk's posture is peaceful, not grieving—hands behind back, chin tilted toward the heavens. The full moon hangs enormous and silver-white, casting soft pale light across the scene, its reflection creating subtle highlights on the monk's bald head and shoulders. Scattered wooden beams and ash suggest the remnants of a structure, but the destruction is understated—a suggestion rather than spectacle. Motion is extremely subtle and cyclical: gentle drift of smoke wisps upward, barely perceptible flicker in the dying embers, soft pulse of moonlight. The monk remains still as stone, a figure of profound acceptance. The scene returns naturally to the starting state, allowing seamless looping. Flat color planes: deep indigo and midnight blue for the night sky, warm amber and soft coral for the distant ember glow, silver-white and pale blue for moonlight, muted earth tones and charcoal grey for the monk and ruins. Visible woodblock texture, delicate ink outlines, washi paper grain, organic imperfections. Asymmetrical yet vertically balanced composition with the moon as focal point in the upper third, generous negative space in the surrounding darkness. Calm, meditative, contemplative mood with a quiet sense of wonder rather than tragedy. Edo-period Japanese woodcut aesthetic, ukiyo-e style. No photorealism, no camera movement, no depth-of-field effects, no cinematic lighting, no modern elements, no 3D.\n",
      "  negative_prompt: \n",
      "  seed: 42\n",
      "  device_id: 0\n",
      "  offload_models: False\n",
      "  keep_both_loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Basic Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'checkpoint_dir_14B': '/workspace/wan2.1/Wan2.1/Wan2.1-T2V-14B',\n",
    "    'checkpoint_dir_1_3B': '/workspace/wan2.1/Wan2.1/Wan2.1-T2V-1.3B',\n",
    "    'output_dir': '/workspace/wan2.1/Wan2.1/outputs',\n",
    "    \n",
    "    # Resolution (480p)\n",
    "    'width': 832,\n",
    "    'height': 480,\n",
    "    'frame_num': 81,  # Must be 4n+1\n",
    "    \n",
    "    # Sampling parameters\n",
    "    'total_sampling_steps': 50,\n",
    "    'sample_solver': 'unipc',  # 'unipc' or 'dpm++'\n",
    "    'shift': 5.0,\n",
    "    'guide_scale': 5.0,\n",
    "    \n",
    "    # Prompt\n",
    "    #'prompt': \"\"\"A serene Japanese ukiyo-e woodblock print style animation. An elderly Zen monk in simple patched robes stands at the edge of frame, his back slightly turned, gazing upward at a luminous full moon in a beautiful night sky that dominates the upper portion of the composition. Behind him, the faint orange glow of dying embers marks where a small hut once stood—wisps of smoke curl lazily upward(smoke should be moving lightly), dissolving into the deep indigo night sky. The monk's posture is peaceful, not grieving—hands behind back, chin tilted toward the heavens. The full moon hangs enormous and silver-white, casting soft pale light across the scene, its reflection creating subtle highlights on the monk's bald head and shoulders. Scattered wooden beams and ash suggest the remnants of a structure, but the destruction is understated—a suggestion rather than spectacle. Motion is extremely subtle and cyclical: gentle drift of smoke wisps upward, barely perceptible flicker in the dying embers, soft pulse of moonlight. The monk remains still as stone, a figure of profound acceptance. The scene returns naturally to the starting state, allowing seamless looping. Flat color planes: deep indigo and midnight blue for the night sky, warm amber and soft coral for the distant ember glow, silver-white and pale blue for moonlight, muted earth tones and charcoal grey for the monk and ruins. Visible woodblock texture, delicate ink outlines, washi paper grain, organic imperfections. Asymmetrical yet vertically balanced composition with the moon as focal point in the upper third, generous negative space in the surrounding darkness. Calm, meditative, contemplative mood with a quiet sense of wonder rather than tragedy. Edo-period Japanese woodcut aesthetic, ukiyo-e style. No photorealism, no camera movement, no depth-of-field effects, no cinematic lighting, no modern elements, no 3D.\"\"\",\n",
    "    'prompt': 'Two anthropomorphic cats in comfy boxing gear sparring playfully in a cozy living room',\n",
    "    'negative_prompt': '',  # Will use default if empty\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Device\n",
    "    'device_id': 0,\n",
    "    \n",
    "    # Model management\n",
    "    'offload_models': False,  # Set to True to offload models between segments (saves memory)\n",
    "    'keep_both_loaded': True,  # Set to False if running into memory issues\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Schedule Configuration\n",
    "\n",
    "Configure the hybrid sampling schedule here. Define which steps use which model.\n",
    "\n",
    "**Patterns:**\n",
    "- **LSL**: Large → Small → Large (e.g., 3-44-3 for 50 steps)\n",
    "- **LSSSL**: Large → Small → Small → Small → Large (e.g., 10-10-10-10-10)\n",
    "- **Custom**: Define your own segment boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Schedule:\n",
      "--------------------------------------------------\n",
      "Segment 1: Steps  0-20 (20 steps) → 14B\n",
      "Segment 2: Steps 20-50 (30 steps) → 1.3B\n",
      "--------------------------------------------------\n",
      "Total: 50 steps\n"
     ]
    }
   ],
   "source": [
    "# Sampling Schedule Configuration\n",
    "# Define segments: list of (model_name, num_steps)\n",
    "# model_name: '14B' or '1.3B'\n",
    "\n",
    "# Example patterns (uncomment one or create your own):\n",
    "\n",
    "# Pattern 1: LSL (3-44-3) - Original request\n",
    "SAMPLING_SCHEDULE = [\n",
    "    ('14B', 20),\n",
    "    ('1.3B', 30), \n",
    "        # First 3 steps with 14B\n",
    "    \n",
    "]\n",
    "\n",
    "# Pattern 2: LSSSL (10-10-10-10-10)\n",
    "# SAMPLING_SCHEDULE = [\n",
    "#     ('14B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('14B', 10),\n",
    "# ]\n",
    "\n",
    "# Pattern 3: Heavy start and end (5-40-5)\n",
    "# SAMPLING_SCHEDULE = [\n",
    "#     ('14B', 5),\n",
    "#     ('1.3B', 40),\n",
    "#     ('14B', 5),\n",
    "# ]\n",
    "\n",
    "# Pattern 4: Multiple switches (5-10-10-10-10-5)\n",
    "# SAMPLING_SCHEDULE = [\n",
    "#     ('14B', 5),\n",
    "#     ('1.3B', 10),\n",
    "#     ('14B', 10),\n",
    "#     ('1.3B', 10),\n",
    "#     ('14B', 10),\n",
    "#     ('1.3B', 5),\n",
    "# ]\n",
    "\n",
    "# Validate schedule\n",
    "total_steps_scheduled = sum(steps for _, steps in SAMPLING_SCHEDULE)\n",
    "assert total_steps_scheduled == CONFIG['total_sampling_steps'], \\\n",
    "    f\"Schedule steps ({total_steps_scheduled}) must match total_sampling_steps ({CONFIG['total_sampling_steps']})\"\n",
    "\n",
    "print(\"Sampling Schedule:\")\n",
    "print(\"-\" * 50)\n",
    "cumulative = 0\n",
    "for i, (model, steps) in enumerate(SAMPLING_SCHEDULE):\n",
    "    start_step = cumulative\n",
    "    end_step = cumulative + steps\n",
    "    cumulative = end_step\n",
    "    print(f\"Segment {i+1}: Steps {start_step:2d}-{end_step:2d} ({steps:2d} steps) → {model}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total: {total_steps_scheduled} steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Utility Functions for Profiling\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in GB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / 1e9,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1e9,\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / 1e9,\n",
    "        }\n",
    "    return {'allocated': 0, 'reserved': 0, 'max_allocated': 0}\n",
    "\n",
    "def reset_peak_memory():\n",
    "    \"\"\"Reset peak memory stats\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "class SegmentProfiler:\n",
    "    \"\"\"Profile a sampling segment\"\"\"\n",
    "    def __init__(self, segment_name: str):\n",
    "        self.segment_name = segment_name\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.start_memory = None\n",
    "        self.peak_memory = None\n",
    "        self.step_times = []\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start profiling\"\"\"\n",
    "        reset_peak_memory()\n",
    "        self.start_time = time.time()\n",
    "        self.start_memory = get_gpu_memory()\n",
    "        \n",
    "    def record_step(self, step_time: float):\n",
    "        \"\"\"Record time for a single step\"\"\"\n",
    "        self.step_times.append(step_time)\n",
    "        \n",
    "    def end(self):\n",
    "        \"\"\"End profiling\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        self.peak_memory = get_gpu_memory()\n",
    "        \n",
    "    def get_report(self) -> Dict:\n",
    "        \"\"\"Get profiling report\"\"\"\n",
    "        total_time = self.end_time - self.start_time if self.end_time else 0\n",
    "        avg_step_time = np.mean(self.step_times) if self.step_times else 0\n",
    "        \n",
    "        return {\n",
    "            'segment_name': self.segment_name,\n",
    "            'num_steps': len(self.step_times),\n",
    "            'total_time': total_time,\n",
    "            'avg_step_time': avg_step_time,\n",
    "            'min_step_time': np.min(self.step_times) if self.step_times else 0,\n",
    "            'max_step_time': np.max(self.step_times) if self.step_times else 0,\n",
    "            'memory_start_allocated_gb': self.start_memory['allocated'],\n",
    "            'memory_peak_allocated_gb': self.peak_memory['max_allocated'],\n",
    "            'memory_peak_reserved_gb': self.peak_memory['reserved'],\n",
    "        }\n",
    "        \n",
    "    def print_report(self):\n",
    "        \"\"\"Print formatted report\"\"\"\n",
    "        report = self.get_report()\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Segment: {report['segment_name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Steps: {report['num_steps']}\")\n",
    "        print(f\"Total time: {report['total_time']:.2f}s\")\n",
    "        print(f\"Avg step time: {report['avg_step_time']:.3f}s\")\n",
    "        print(f\"Min step time: {report['min_step_time']:.3f}s\")\n",
    "        print(f\"Max step time: {report['max_step_time']:.3f}s\")\n",
    "        print(f\"Memory (start): {report['memory_start_allocated_gb']:.2f} GB\")\n",
    "        print(f\"Memory (peak): {report['memory_peak_allocated_gb']:.2f} GB\")\n",
    "        print(f\"Memory (reserved): {report['memory_peak_reserved_gb']:.2f} GB\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"✓ Utility functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "Load both 14B and 1.3B models. They share the same VAE and T5 encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 14B model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 14B model loaded\n",
      "Memory after 14B: 121.96 GB\n",
      "\n",
      "Loading 1.3B model...\n",
      "✓ 1.3B model loaded\n",
      "Memory after 1.3B: 128.15 GB\n",
      "\n",
      "✓ Both models loaded. Total memory: 64.25 GB\n"
     ]
    }
   ],
   "source": [
    "# Load 14B Model\n",
    "print(\"Loading 14B model...\")\n",
    "model_14B = WanT2V(\n",
    "    config=t2v_14B,\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir_14B'],\n",
    "    device_id=CONFIG['device_id'],\n",
    "    rank=0,\n",
    "    t5_fsdp=False,\n",
    "    dit_fsdp=False,\n",
    "    use_usp=False,\n",
    "    t5_cpu=False,\n",
    ")\n",
    "print(\"✓ 14B model loaded\")\n",
    "print(f\"Memory after 14B: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "\n",
    "# Load 1.3B Model\n",
    "print(\"\\nLoading 1.3B model...\")\n",
    "model_1_3B = WanT2V(\n",
    "    config=t2v_1_3B,\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir_1_3B'],\n",
    "    device_id=CONFIG['device_id'],\n",
    "    rank=0,\n",
    "    t5_fsdp=False,\n",
    "    dit_fsdp=False,\n",
    "    use_usp=False,\n",
    "    t5_cpu=False,\n",
    ")\n",
    "print(\"✓ 1.3B model loaded\")\n",
    "print(f\"Memory after 1.3B: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "\n",
    "# Store models in a dictionary for easy access\n",
    "models = {\n",
    "    '14B': model_14B,\n",
    "    '1.3B': model_1_3B,\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Both models loaded. Total memory: {get_gpu_memory()['allocated']:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Sampling\n",
    "\n",
    "FlowBending-inspired hybrid sampling with model switching in latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hybrid sampling function defined\n"
     ]
    }
   ],
   "source": [
    "def hybrid_generate(\n",
    "    models: Dict,\n",
    "    sampling_schedule: List[Tuple[str, int]],\n",
    "    config: Dict,\n",
    "    profile: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate video using hybrid model sampling.\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of models {'14B': model_14B, '1.3B': model_1_3B}\n",
    "        sampling_schedule: List of (model_name, num_steps) tuples\n",
    "        config: Configuration dictionary\n",
    "        profile: Whether to profile each segment\n",
    "        \n",
    "    Returns:\n",
    "        video: Generated video tensor\n",
    "        profiling_reports: List of profiling reports for each segment\n",
    "    \"\"\"\n",
    "    device = torch.device(f\"cuda:{config['device_id']}\")\n",
    "    \n",
    "    # Use first model to get shared components\n",
    "    first_model = models[sampling_schedule[0][0]]\n",
    "    \n",
    "    # Prepare latent shape\n",
    "    F = config['frame_num']\n",
    "    size = (config['width'], config['height'])\n",
    "    vae_stride = first_model.vae_stride\n",
    "    patch_size = first_model.patch_size\n",
    "    \n",
    "    target_shape = (\n",
    "        first_model.vae.model.z_dim,\n",
    "        (F - 1) // vae_stride[0] + 1,\n",
    "        size[1] // vae_stride[1],\n",
    "        size[0] // vae_stride[2]\n",
    "    )\n",
    "    \n",
    "    seq_len = math.ceil(\n",
    "        (target_shape[2] * target_shape[3]) / (patch_size[1] * patch_size[2]) * target_shape[1]\n",
    "    )\n",
    "    \n",
    "    # Setup text encoding (shared across all models)\n",
    "    n_prompt = config['negative_prompt'] if config['negative_prompt'] else first_model.sample_neg_prompt\n",
    "    seed = config['seed'] if config['seed'] >= 0 else random.randint(0, sys.maxsize)\n",
    "    \n",
    "    print(f\"Using seed: {seed}\")\n",
    "    print(f\"Target latent shape: {target_shape}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    \n",
    "    # Encode text prompt (use T5 from first model)\n",
    "    first_model.text_encoder.model.to(device)\n",
    "    context = first_model.text_encoder([config['prompt']], device)\n",
    "    context_null = first_model.text_encoder([n_prompt], device)\n",
    "    first_model.text_encoder.model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize noise with seed\n",
    "    seed_g = torch.Generator(device=device)\n",
    "    seed_g.manual_seed(seed)\n",
    "    \n",
    "    noise = torch.randn(\n",
    "        target_shape[0], target_shape[1], target_shape[2], target_shape[3],\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "        generator=seed_g\n",
    "    )\n",
    "    \n",
    "    # Setup scheduler\n",
    "    num_train_timesteps = first_model.num_train_timesteps\n",
    "    if config['sample_solver'] == 'unipc':\n",
    "        sample_scheduler = FlowUniPCMultistepScheduler(\n",
    "            num_train_timesteps=num_train_timesteps,\n",
    "            shift=1,\n",
    "            use_dynamic_shifting=False\n",
    "        )\n",
    "        sample_scheduler.set_timesteps(config['total_sampling_steps'], device=device, shift=config['shift'])\n",
    "        timesteps = sample_scheduler.timesteps\n",
    "    elif config['sample_solver'] == 'dpm++':\n",
    "        sample_scheduler = FlowDPMSolverMultistepScheduler(\n",
    "            num_train_timesteps=num_train_timesteps,\n",
    "            shift=1,\n",
    "            use_dynamic_shifting=False\n",
    "        )\n",
    "        sampling_sigmas = get_sampling_sigmas(config['total_sampling_steps'], config['shift'])\n",
    "        timesteps, _ = retrieve_timesteps(sample_scheduler, device=device, sigmas=sampling_sigmas)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported solver: {config['sample_solver']}\")\n",
    "    \n",
    "    print(f\"Total timesteps: {len(timesteps)}\")\n",
    "    \n",
    "    # Initialize latents\n",
    "    latents = noise\n",
    "    \n",
    "    # Prepare model arguments\n",
    "    arg_c = {'context': context, 'seq_len': seq_len}\n",
    "    arg_null = {'context': context_null, 'seq_len': seq_len}\n",
    "    \n",
    "    # Run hybrid sampling\n",
    "    profiling_reports = []\n",
    "    step_idx = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYBRID SAMPLING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for segment_idx, (model_name, num_steps) in enumerate(sampling_schedule):\n",
    "        model = models[model_name]\n",
    "        segment_name = f\"Segment {segment_idx+1}: {model_name} ({num_steps} steps)\"\n",
    "        \n",
    "        print(f\"\\n{segment_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Setup profiler\n",
    "        profiler = SegmentProfiler(segment_name) if profile else None\n",
    "        if profiler:\n",
    "            profiler.start()\n",
    "        \n",
    "        # Move model to device if needed\n",
    "        model.model.to(device)\n",
    "        \n",
    "        # Sample for this segment\n",
    "        segment_timesteps = timesteps[step_idx:step_idx + num_steps]\n",
    "        \n",
    "        with amp.autocast(dtype=model.param_dtype), torch.no_grad():\n",
    "            for i, t in enumerate(tqdm(segment_timesteps, desc=model_name)):\n",
    "                step_start = time.time()\n",
    "                \n",
    "                latent_model_input = [latents]\n",
    "                timestep = torch.stack([t])\n",
    "                \n",
    "                # Conditional prediction\n",
    "                noise_pred_cond = model.model(latent_model_input, t=timestep, **arg_c)[0]\n",
    "                # Unconditional prediction\n",
    "                noise_pred_uncond = model.model(latent_model_input, t=timestep, **arg_null)[0]\n",
    "                \n",
    "                # Classifier-free guidance\n",
    "                noise_pred = noise_pred_uncond + config['guide_scale'] * (noise_pred_cond - noise_pred_uncond)\n",
    "                \n",
    "                # Scheduler step\n",
    "                temp_x0 = sample_scheduler.step(\n",
    "                    noise_pred.unsqueeze(0),\n",
    "                    t,\n",
    "                    latents.unsqueeze(0),\n",
    "                    return_dict=False,\n",
    "                    generator=seed_g\n",
    "                )[0]\n",
    "                latents = temp_x0.squeeze(0)\n",
    "                \n",
    "                step_time = time.time() - step_start\n",
    "                if profiler:\n",
    "                    profiler.record_step(step_time)\n",
    "        \n",
    "        # End profiling\n",
    "        if profiler:\n",
    "            profiler.end()\n",
    "            profiler.print_report()\n",
    "            profiling_reports.append(profiler.get_report())\n",
    "        \n",
    "        # Offload model if requested\n",
    "        if config['offload_models']:\n",
    "            model.model.cpu()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Offloaded {model_name} model\")\n",
    "        \n",
    "        step_idx += num_steps\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DECODING LATENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Decode latents using VAE (from any model, they're the same)\n",
    "    decode_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        videos = first_model.vae.decode([latents])\n",
    "    decode_time = time.time() - decode_start\n",
    "    print(f\"Decode time: {decode_time:.2f}s\")\n",
    "    \n",
    "    return videos[0], profiling_reports\n",
    "\n",
    "print(\"✓ Hybrid sampling function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hybrid Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hybrid generation...\n",
      "Prompt: A serene Japanese ukiyo-e woodblock print style animation. An elderly Zen monk in simple patched robes stands at the edge of frame, his back slightly turned, gazing upward at a luminous full moon in a beautiful night sky that dominates the upper portion of the composition. Behind him, the faint orange glow of dying embers marks where a small hut once stood—wisps of smoke curl lazily upward(smoke should be moving lightly), dissolving into the deep indigo night sky. The monk's posture is peaceful, not grieving—hands behind back, chin tilted toward the heavens. The full moon hangs enormous and silver-white, casting soft pale light across the scene, its reflection creating subtle highlights on the monk's bald head and shoulders. Scattered wooden beams and ash suggest the remnants of a structure, but the destruction is understated—a suggestion rather than spectacle. Motion is extremely subtle and cyclical: gentle drift of smoke wisps upward, barely perceptible flicker in the dying embers, soft pulse of moonlight. The monk remains still as stone, a figure of profound acceptance. The scene returns naturally to the starting state, allowing seamless looping. Flat color planes: deep indigo and midnight blue for the night sky, warm amber and soft coral for the distant ember glow, silver-white and pale blue for moonlight, muted earth tones and charcoal grey for the monk and ruins. Visible woodblock texture, delicate ink outlines, washi paper grain, organic imperfections. Asymmetrical yet vertically balanced composition with the moon as focal point in the upper third, generous negative space in the surrounding darkness. Calm, meditative, contemplative mood with a quiet sense of wonder rather than tragedy. Edo-period Japanese woodcut aesthetic, ukiyo-e style. No photorealism, no camera movement, no depth-of-field effects, no cinematic lighting, no modern elements, no 3D.\n",
      "Resolution: 832x480\n",
      "Frames: 81\n",
      "Using seed: 42\n",
      "Target latent shape: (16, 21, 60, 104)\n",
      "Sequence length: 32760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4218/474209478.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(dtype=model.param_dtype), torch.no_grad():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 50\n",
      "\n",
      "================================================================================\n",
      "HYBRID SAMPLING\n",
      "================================================================================\n",
      "\n",
      "Segment 1: 14B (20 steps)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14B: 100%|██████████| 20/20 [02:12<00:00,  6.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Segment: Segment 1: 14B (20 steps)\n",
      "============================================================\n",
      "Steps: 20\n",
      "Total time: 132.90s\n",
      "Avg step time: 6.643s\n",
      "Min step time: 6.629s\n",
      "Max step time: 6.657s\n",
      "Memory (start): 64.27 GB\n",
      "Memory (peak): 71.42 GB\n",
      "Memory (reserved): 73.12 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "Segment 2: 1.3B (30 steps)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.3B: 100%|██████████| 30/30 [00:43<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Segment: Segment 2: 1.3B (30 steps)\n",
      "============================================================\n",
      "Steps: 30\n",
      "Total time: 43.14s\n",
      "Avg step time: 1.437s\n",
      "Min step time: 1.436s\n",
      "Max step time: 1.439s\n",
      "Memory (start): 64.34 GB\n",
      "Memory (peak): 66.48 GB\n",
      "Memory (reserved): 73.12 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DECODING LATENTS\n",
      "================================================================================\n",
      "Decode time: 1.43s\n",
      "\n",
      "✓ Hybrid video saved to: /workspace/wan2.1/Wan2.1/outputs/hybrid_832x480_20260106_163221.mp4\n"
     ]
    }
   ],
   "source": [
    "# Run Hybrid Generation\n",
    "print(\"Starting hybrid generation...\")\n",
    "print(f\"Prompt: {CONFIG['prompt']}\")\n",
    "print(f\"Resolution: {CONFIG['width']}x{CONFIG['height']}\")\n",
    "print(f\"Frames: {CONFIG['frame_num']}\")\n",
    "\n",
    "hybrid_video, hybrid_reports = hybrid_generate(\n",
    "    models=models,\n",
    "    sampling_schedule=SAMPLING_SCHEDULE,\n",
    "    config=CONFIG,\n",
    "    profile=True\n",
    ")\n",
    "\n",
    "# Save hybrid video\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hybrid_output_path = os.path.join(\n",
    "    CONFIG['output_dir'],\n",
    "    f\"hybrid_{CONFIG['width']}x{CONFIG['height']}_{timestamp}.mp4\"\n",
    ")\n",
    "cache_video(hybrid_video[None], save_file=hybrid_output_path, fps=16, nrow=1, normalize=True, value_range=(-1, 1))\n",
    "print(f\"\\n✓ Hybrid video saved to: {hybrid_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: 14B-Only Generation\n",
    "\n",
    "Run full 50-step generation with 14B model only for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 14B baseline generation...\n",
      "Prompt: A serene Japanese ukiyo-e woodblock print style animation. An elderly Zen monk in simple patched robes stands at the edge of frame, his back slightly turned, gazing upward at a luminous full moon in a beautiful night sky that dominates the upper portion of the composition. Behind him, the faint orange glow of dying embers marks where a small hut once stood—wisps of smoke curl lazily upward(smoke should be moving lightly), dissolving into the deep indigo night sky. The monk's posture is peaceful, not grieving—hands behind back, chin tilted toward the heavens. The full moon hangs enormous and silver-white, casting soft pale light across the scene, its reflection creating subtle highlights on the monk's bald head and shoulders. Scattered wooden beams and ash suggest the remnants of a structure, but the destruction is understated—a suggestion rather than spectacle. Motion is extremely subtle and cyclical: gentle drift of smoke wisps upward, barely perceptible flicker in the dying embers, soft pulse of moonlight. The monk remains still as stone, a figure of profound acceptance. The scene returns naturally to the starting state, allowing seamless looping. Flat color planes: deep indigo and midnight blue for the night sky, warm amber and soft coral for the distant ember glow, silver-white and pale blue for moonlight, muted earth tones and charcoal grey for the monk and ruins. Visible woodblock texture, delicate ink outlines, washi paper grain, organic imperfections. Asymmetrical yet vertically balanced composition with the moon as focal point in the upper third, generous negative space in the surrounding darkness. Calm, meditative, contemplative mood with a quiet sense of wonder rather than tragedy. Edo-period Japanese woodcut aesthetic, ukiyo-e style. No photorealism, no camera movement, no depth-of-field effects, no cinematic lighting, no modern elements, no 3D.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wan2.1/Wan2.1/wan/text2video.py:204: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(dtype=self.param_dtype), torch.no_grad(), no_sync():\n",
      "100%|██████████| 50/50 [05:32<00:00,  6.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 14B baseline completed in 336.61s\n",
      "✓ Baseline video saved to: /workspace/wan2.1/Wan2.1/outputs/baseline_14B_832x480_20260106_163221.mp4\n"
     ]
    }
   ],
   "source": [
    "# Generate baseline with 14B only\n",
    "print(\"Starting 14B baseline generation...\")\n",
    "print(f\"Prompt: {CONFIG['prompt']}\")\n",
    "\n",
    "baseline_start = time.time()\n",
    "\n",
    "baseline_video = model_14B.generate(\n",
    "    input_prompt=CONFIG['prompt'],\n",
    "    size=(CONFIG['width'], CONFIG['height']),\n",
    "    frame_num=CONFIG['frame_num'],\n",
    "    shift=CONFIG['shift'],\n",
    "    sample_solver=CONFIG['sample_solver'],\n",
    "    sampling_steps=CONFIG['total_sampling_steps'],\n",
    "    guide_scale=CONFIG['guide_scale'],\n",
    "    n_prompt=CONFIG['negative_prompt'],\n",
    "    seed=CONFIG['seed'],\n",
    "    offload_model=CONFIG['offload_models']\n",
    ")\n",
    "\n",
    "baseline_time = time.time() - baseline_start\n",
    "print(f\"\\n✓ 14B baseline completed in {baseline_time:.2f}s\")\n",
    "\n",
    "# Save baseline video\n",
    "baseline_output_path = os.path.join(\n",
    "    CONFIG['output_dir'],\n",
    "    f\"baseline_14B_{CONFIG['width']}x{CONFIG['height']}_{timestamp}.mp4\"\n",
    ")\n",
    "cache_video(baseline_video[None], save_file=baseline_output_path, fps=16, nrow=1, normalize=True, value_range=(-1, 1))\n",
    "print(f\"✓ Baseline video saved to: {baseline_output_path}\")\n",
    "\n",
    "# Create baseline report\n",
    "baseline_report = {\n",
    "    'model': '14B only',\n",
    "    'total_steps': CONFIG['total_sampling_steps'],\n",
    "    'total_time': baseline_time,\n",
    "    'avg_step_time': baseline_time / CONFIG['total_sampling_steps'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: 1.3B-Only Generation\n",
    "\n",
    "Run full 50-step generation with 1.3B model only for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1.3B baseline generation...\n",
      "Prompt: A serene Japanese ukiyo-e woodblock print style animation. An elderly Zen monk in simple patched robes stands at the edge of frame, his back slightly turned, gazing upward at a luminous full moon in a beautiful night sky that dominates the upper portion of the composition. Behind him, the faint orange glow of dying embers marks where a small hut once stood—wisps of smoke curl lazily upward(smoke should be moving lightly), dissolving into the deep indigo night sky. The monk's posture is peaceful, not grieving—hands behind back, chin tilted toward the heavens. The full moon hangs enormous and silver-white, casting soft pale light across the scene, its reflection creating subtle highlights on the monk's bald head and shoulders. Scattered wooden beams and ash suggest the remnants of a structure, but the destruction is understated—a suggestion rather than spectacle. Motion is extremely subtle and cyclical: gentle drift of smoke wisps upward, barely perceptible flicker in the dying embers, soft pulse of moonlight. The monk remains still as stone, a figure of profound acceptance. The scene returns naturally to the starting state, allowing seamless looping. Flat color planes: deep indigo and midnight blue for the night sky, warm amber and soft coral for the distant ember glow, silver-white and pale blue for moonlight, muted earth tones and charcoal grey for the monk and ruins. Visible woodblock texture, delicate ink outlines, washi paper grain, organic imperfections. Asymmetrical yet vertically balanced composition with the moon as focal point in the upper third, generous negative space in the surrounding darkness. Calm, meditative, contemplative mood with a quiet sense of wonder rather than tragedy. Edo-period Japanese woodcut aesthetic, ukiyo-e style. No photorealism, no camera movement, no depth-of-field effects, no cinematic lighting, no modern elements, no 3D.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:11<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 1.3B baseline completed in 77.21s\n",
      "✓ 1.3B Baseline video saved to: /workspace/wan2.1/Wan2.1/outputs/baseline_1.3B_832x480_20260106_163221.mp4\n"
     ]
    }
   ],
   "source": [
    "# Generate baseline with 1.3B only\n",
    "print(\"Starting 1.3B baseline generation...\")\n",
    "print(f\"Prompt: {CONFIG['prompt']}\")\n",
    "\n",
    "baseline_1_3B_start = time.time()\n",
    "\n",
    "baseline_1_3B_video = model_1_3B.generate(\n",
    "    input_prompt=CONFIG['prompt'],\n",
    "    size=(CONFIG['width'], CONFIG['height']),\n",
    "    frame_num=CONFIG['frame_num'],\n",
    "    shift=CONFIG['shift'],\n",
    "    sample_solver=CONFIG['sample_solver'],\n",
    "    sampling_steps=CONFIG['total_sampling_steps'],\n",
    "    guide_scale=CONFIG['guide_scale'],\n",
    "    n_prompt=CONFIG['negative_prompt'],\n",
    "    seed=CONFIG['seed'],\n",
    "    offload_model=CONFIG['offload_models']\n",
    ")\n",
    "\n",
    "baseline_1_3B_time = time.time() - baseline_1_3B_start\n",
    "print(f\"\\n✓ 1.3B baseline completed in {baseline_1_3B_time:.2f}s\")\n",
    "\n",
    "# Save baseline video\n",
    "baseline_1_3B_output_path = os.path.join(\n",
    "    CONFIG['output_dir'],\n",
    "    f\"baseline_1.3B_{CONFIG['width']}x{CONFIG['height']}_{timestamp}.mp4\"\n",
    ")\n",
    "cache_video(baseline_1_3B_video[None], save_file=baseline_1_3B_output_path, fps=16, nrow=1, normalize=True, value_range=(-1, 1))\n",
    "print(f\"✓ 1.3B Baseline video saved to: {baseline_1_3B_output_path}\")\n",
    "\n",
    "# Create baseline report\n",
    "baseline_1_3B_report = {\n",
    "    'model': '1.3B only',\n",
    "    'total_steps': CONFIG['total_sampling_steps'],\n",
    "    'total_time': baseline_1_3B_time,\n",
    "    'avg_step_time': baseline_1_3B_time / CONFIG['total_sampling_steps'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Report\n",
    "\n",
    "Generate comprehensive profiling report comparing hybrid vs baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROFILING REPORT - HYBRID VS BASELINES\n",
      "================================================================================\n",
      "\n",
      "### HYBRID MODEL SAMPLING ###\n",
      "Schedule: [('14B', 20), ('1.3B', 30)]\n",
      "\n",
      "Segment Details:\n",
      "\n",
      "  Segment 1: 14B (20 steps)\n",
      "    Steps: 20\n",
      "    Total time: 132.90s\n",
      "    Avg step time: 6.643s\n",
      "    Peak memory: 71.42 GB\n",
      "\n",
      "  Segment 2: 1.3B (30 steps)\n",
      "    Steps: 30\n",
      "    Total time: 43.14s\n",
      "    Avg step time: 1.437s\n",
      "    Peak memory: 66.48 GB\n",
      "\n",
      "  Hybrid Total Time: 176.03s\n",
      "  Hybrid Avg Step Time: 3.521s\n",
      "\n",
      "### BASELINE: 14B ONLY ###\n",
      "Total time: 336.61s\n",
      "Avg step time: 6.732s\n",
      "\n",
      "### BASELINE: 1.3B ONLY ###\n",
      "Total time: 77.21s\n",
      "Avg step time: 1.544s\n",
      "\n",
      "### COMPARISON: HYBRID vs 14B BASELINE ###\n",
      "Speedup: 1.91x\n",
      "Time saved: 160.58s\n",
      "Percentage faster: 47.7%\n",
      "\n",
      "### COMPARISON: HYBRID vs 1.3B BASELINE ###\n",
      "Slowdown: 2.28x (hybrid is slower)\n",
      "Extra time: 98.82s\n",
      "Percentage slower: 128.0%\n",
      "\n",
      "### QUALITY vs SPEED TRADE-OFF ###\n",
      "14B baseline: 336.61s (highest quality)\n",
      "Hybrid model: 176.03s (balanced)\n",
      "1.3B baseline: 77.21s (fastest)\n",
      "\n",
      "Hybrid saves 47.7% time vs 14B\n",
      "Hybrid adds 128.0% time vs 1.3B\n",
      "\n",
      "✓ Report saved to: /workspace/wan2.1/Wan2.1/outputs/profiling_report_20260106_163221.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROFILING REPORT - HYBRID VS BASELINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hybrid summary\n",
    "print(\"\\n### HYBRID MODEL SAMPLING ###\")\n",
    "print(f\"Schedule: {SAMPLING_SCHEDULE}\")\n",
    "print(f\"\\nSegment Details:\")\n",
    "hybrid_total_time = 0\n",
    "for report in hybrid_reports:\n",
    "    print(f\"\\n  {report['segment_name']}\")\n",
    "    print(f\"    Steps: {report['num_steps']}\")\n",
    "    print(f\"    Total time: {report['total_time']:.2f}s\")\n",
    "    print(f\"    Avg step time: {report['avg_step_time']:.3f}s\")\n",
    "    print(f\"    Peak memory: {report['memory_peak_allocated_gb']:.2f} GB\")\n",
    "    hybrid_total_time += report['total_time']\n",
    "\n",
    "print(f\"\\n  Hybrid Total Time: {hybrid_total_time:.2f}s\")\n",
    "print(f\"  Hybrid Avg Step Time: {hybrid_total_time / CONFIG['total_sampling_steps']:.3f}s\")\n",
    "\n",
    "# Baseline summaries\n",
    "print(\"\\n### BASELINE: 14B ONLY ###\")\n",
    "print(f\"Total time: {baseline_report['total_time']:.2f}s\")\n",
    "print(f\"Avg step time: {baseline_report['avg_step_time']:.3f}s\")\n",
    "\n",
    "print(\"\\n### BASELINE: 1.3B ONLY ###\")\n",
    "print(f\"Total time: {baseline_1_3B_report['total_time']:.2f}s\")\n",
    "print(f\"Avg step time: {baseline_1_3B_report['avg_step_time']:.3f}s\")\n",
    "\n",
    "# Comparisons\n",
    "speedup_vs_14B = baseline_report['total_time'] / hybrid_total_time\n",
    "speedup_vs_1_3B = baseline_1_3B_report['total_time'] / hybrid_total_time\n",
    "\n",
    "print(\"\\n### COMPARISON: HYBRID vs 14B BASELINE ###\")\n",
    "print(f\"Speedup: {speedup_vs_14B:.2f}x\")\n",
    "print(f\"Time saved: {baseline_report['total_time'] - hybrid_total_time:.2f}s\")\n",
    "print(f\"Percentage faster: {(1 - 1/speedup_vs_14B) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n### COMPARISON: HYBRID vs 1.3B BASELINE ###\")\n",
    "if speedup_vs_1_3B > 1:\n",
    "    print(f\"Speedup: {speedup_vs_1_3B:.2f}x\")\n",
    "    print(f\"Time saved: {baseline_1_3B_report['total_time'] - hybrid_total_time:.2f}s\")\n",
    "    print(f\"Percentage faster: {(1 - 1/speedup_vs_1_3B) * 100:.1f}%\")\n",
    "else:\n",
    "    slowdown = 1 / speedup_vs_1_3B\n",
    "    print(f\"Slowdown: {slowdown:.2f}x (hybrid is slower)\")\n",
    "    print(f\"Extra time: {hybrid_total_time - baseline_1_3B_report['total_time']:.2f}s\")\n",
    "    print(f\"Percentage slower: {(slowdown - 1) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n### QUALITY vs SPEED TRADE-OFF ###\")\n",
    "print(f\"14B baseline: {baseline_report['total_time']:.2f}s (highest quality)\")\n",
    "print(f\"Hybrid model: {hybrid_total_time:.2f}s (balanced)\")\n",
    "print(f\"1.3B baseline: {baseline_1_3B_report['total_time']:.2f}s (fastest)\")\n",
    "print(f\"\\nHybrid saves {(1 - hybrid_total_time/baseline_report['total_time']) * 100:.1f}% time vs 14B\")\n",
    "print(f\"Hybrid adds {(hybrid_total_time/baseline_1_3B_report['total_time'] - 1) * 100:.1f}% time vs 1.3B\")\n",
    "\n",
    "# Save report to JSON\n",
    "report_data = {\n",
    "    'timestamp': timestamp,\n",
    "    'config': CONFIG,\n",
    "    'sampling_schedule': SAMPLING_SCHEDULE,\n",
    "    'hybrid': {\n",
    "        'segments': hybrid_reports,\n",
    "        'total_time': hybrid_total_time,\n",
    "        'avg_step_time': hybrid_total_time / CONFIG['total_sampling_steps'],\n",
    "    },\n",
    "    'baseline_14B': baseline_report,\n",
    "    'baseline_1_3B': baseline_1_3B_report,\n",
    "    'comparison': {\n",
    "        'hybrid_vs_14B': {\n",
    "            'speedup': speedup_vs_14B,\n",
    "            'time_saved': baseline_report['total_time'] - hybrid_total_time,\n",
    "            'percentage_faster': (1 - 1/speedup_vs_14B) * 100,\n",
    "        },\n",
    "        'hybrid_vs_1_3B': {\n",
    "            'speedup': speedup_vs_1_3B,\n",
    "            'time_difference': baseline_1_3B_report['total_time'] - hybrid_total_time,\n",
    "            'percentage_difference': (1 - 1/speedup_vs_1_3B) * 100 if speedup_vs_1_3B > 1 else -(1/speedup_vs_1_3B - 1) * 100,\n",
    "        }\n",
    "    },\n",
    "    'outputs': {\n",
    "        'hybrid_video': hybrid_output_path,\n",
    "        'baseline_14B_video': baseline_output_path,\n",
    "        'baseline_1_3B_video': baseline_1_3B_output_path,\n",
    "    }\n",
    "}\n",
    "\n",
    "report_path = os.path.join(CONFIG['output_dir'], f'profiling_report_{timestamp}.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Report saved to: {report_path}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Cleanup\n",
    "\n",
    "Note: This cell is NOT run by default. Execute manually to clean up memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Cleanup Cell\n",
    "# Set this to True and run manually when you want to clean up\n",
    "RUN_CLEANUP = False\n",
    "\n",
    "if RUN_CLEANUP:\n",
    "    import gc\n",
    "    \n",
    "    print(\"Cleaning up memory...\")\n",
    "    print(f\"Memory before cleanup: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "    \n",
    "    # Delete models\n",
    "    if 'model_14B' in locals():\n",
    "        del model_14B\n",
    "    if 'model_1_3B' in locals():\n",
    "        del model_1_3B\n",
    "    if 'models' in locals():\n",
    "        del models\n",
    "    \n",
    "    # Delete videos\n",
    "    if 'hybrid_video' in locals():\n",
    "        del hybrid_video\n",
    "    if 'baseline_video' in locals():\n",
    "        del baseline_video\n",
    "    if 'baseline_1_3B_video' in locals():\n",
    "        del baseline_1_3B_video\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Memory after cleanup: {get_gpu_memory()['allocated']:.2f} GB\")\n",
    "    print(\"✓ Cleanup complete\")\n",
    "else:\n",
    "    print(\"Cleanup skipped. Set RUN_CLEANUP = True and run this cell manually to clean up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements hybrid video generation using FlowBending framework:\n",
    "\n",
    "### Key Features:\n",
    "1. **Flexible Sampling Schedule**: Configure any pattern (LSL, LSSSL, custom)\n",
    "2. **Latent Space Consistency**: Models switch seamlessly in latent space\n",
    "3. **Comprehensive Profiling**: Memory usage, latency, and total time per segment\n",
    "4. **Dual Baseline Comparison**: Full 14B and 1.3B generations for quality vs speed analysis\n",
    "\n",
    "### Outputs:\n",
    "- Hybrid video (saved to outputs directory)\n",
    "- Baseline 14B video (highest quality, saved to outputs directory)\n",
    "- Baseline 1.3B video (fastest, saved to outputs directory)\n",
    "- JSON profiling report with detailed metrics and comparisons\n",
    "\n",
    "### Usage:\n",
    "1. Adjust `CONFIG` settings (resolution, prompt, seed, etc.)\n",
    "2. Modify `SAMPLING_SCHEDULE` to experiment with different patterns\n",
    "3. Run all cells sequentially\n",
    "4. Review profiling reports and compare all three videos\n",
    "5. (Optional) Run cleanup cell manually when done\n",
    "\n",
    "### Expected Results:\n",
    "- **14B Baseline**: Highest quality, slowest generation\n",
    "- **1.3B Baseline**: Lower quality, fastest generation  \n",
    "- **Hybrid**: Balanced quality/speed trade-off\n",
    "\n",
    "### Next Steps:\n",
    "- Compare video quality: hybrid vs 14B vs 1.3B\n",
    "- Experiment with different sampling schedules to optimize the quality/speed balance\n",
    "- Try different resolutions\n",
    "- Test with various prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wan2.1 .venv)",
   "language": "python",
   "name": "wan2.1-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
